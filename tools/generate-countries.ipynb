{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Note book to create html pages for countries and Kreise in Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files:\n",
    "\n",
    "New strategy (with pelican)\n",
    "\n",
    "- put notebooks into wwwroot/ipynb folder\n",
    "- put html into html wwwroot/folder\n",
    "- pelican files can then go into wwwroot folder\n",
    "\n",
    "Advantages:\n",
    "- cleaner than all in one folder\n",
    "- github can display all files in each subdirectory (there is a limit of 500 files or so)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation strategy for html plots\n",
    "\n",
    "such as https://oscovida.github.io/html/Turkey.html\n",
    "\n",
    "- create notebooks for each country from template (in ipynb folder)\n",
    "- run nbconvert on each notebook to execute it. This creates the html notebook.\n",
    "- at the same time, record in the MetadataRegion directory/class what regions have been processed.\n",
    "- the above execution can be requested again for all regions, but those will be skipped that have \n",
    "  recently (by default within the last 2 hours) been processed.\n",
    "  This is useful to just re-run the command if a calculation time-out of (for an unknown reason).\n",
    "\n",
    "- at any time later, create markdown pages (for pelican) based on that metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "from nbconvert import HTMLExporter\n",
    "from nbconvert.writers import FilesWriter\n",
    "\n",
    "import nbformat\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#  Set to false if you do not want multiprocessing enabled\n",
    "cores = 'auto'\n",
    "\n",
    "if cores == 'auto':\n",
    "    cores = max(1, cpu_count()-1) +1\n",
    "\n",
    "\n",
    "if cores:\n",
    "    print(f'Using {cores} processes')\n",
    "    \n",
    "wwwroot = \"wwwroot\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "from coronavirus import *\n",
    "from coronavirus import MetadataRegion\n",
    "\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)  #  Disable pandas scientific notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning of cache and copying files has moved to\n",
    "\n",
    "- `generate-webpage-clean-setup.py` and \n",
    "- `generate-webpage-clean-setup.sh`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_PREFIX = \"Tracking plots: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, c = fetch_deaths(), fetch_cases()\n",
    "\n",
    "countries = d.index\n",
    "countries2 = c.index\n",
    "assert (countries2 == countries).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_US_cases = fetch_cases_US()\n",
    "data_US_deaths = fetch_deaths_US()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also fetch data from Germany, so it is available later from the cache\n",
    "germany = fetch_data_germany()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_template(templatefile, output_file_name, mappings, wwwroot):\n",
    "    \"\"\"Create concrete *.ipynb file from template\n",
    "    - templatefile: the template with placeholders to be substituted\n",
    "    - mappings: dictiorany with placeholders as keys, and values to be substituted\n",
    "    - output_file_name: name to write modified file to\n",
    "    - wwwroot: directory in which the output file should be written\n",
    "    \"\"\"\n",
    "    # open template\n",
    "    with open(templatefile, \"tr\") as f_template:\n",
    "        template = f_template.read()\n",
    "    for key in mappings:\n",
    "        value = mappings[key]\n",
    "        if value is None:\n",
    "            value = str(None)\n",
    "        template = template.replace(key, value)\n",
    "    with open(os.path.join(wwwroot, output_file_name), \"tw\") as f:\n",
    "        f.write(template)\n",
    "    print(f\"Written file to {output_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_country_name_is_known(name):\n",
    "    d = fetch_deaths()\n",
    "    assert name in d.index, f\"{name} is unknown. Known countries are {sorted(d.index)}\"\n",
    "\n",
    "def germany_check_region_name_is_known(name):\n",
    "    d = fetch_data_germany()\n",
    "    assert name in list(d['Bundesland'].drop_duplicates()), \\\n",
    "        f\"{name} is unknown. Known regions are {sorted(list(d['Bundesland'].drop_duplicates()))}\"\n",
    "\n",
    "def germany_check_subregion_name_is_known(name):\n",
    "    d = fetch_data_germany()\n",
    "    assert name in list(d['Landkreis'].drop_duplicates()), \\\n",
    "        f\"{name} is unknown. Known regions are {sorted(list(d['Landkreis'].drop_duplicates()))}\"\n",
    "\n",
    "germany_check_region_name_is_known(\"Hamburg\") \n",
    "germany_check_subregion_name_is_known(\"SK Hamburg\") \n",
    "\n",
    "    \n",
    "def sanitise(name):\n",
    "    \"\"\"Given a country name as a string, sanitise it for use as URL and filename: \n",
    "    - get rid of spaces, commas\n",
    "    \n",
    "    return cleaned string.\n",
    "    \n",
    "    (Leave umlaute for now)\n",
    "    \"\"\"\n",
    "    s = name.replace(\" \", \"-\")\n",
    "    s = s.replace(\",\", \"-\")\n",
    "    return s\n",
    "    \n",
    "    \n",
    "def get_binder_url(notebook):\n",
    "    \"\"\"Given a notebook name, compute the path\"\"\"\n",
    "    base = \"https://mybinder.org/v2/gh/oscovida/binder/master?filepath=ipynb/\"\n",
    "    return base + notebook.replace(\" \", \"%20\")\n",
    "\n",
    "\n",
    "def create_ipynb_for_country(country, templatename, wwwroot):\n",
    "    \"\"\"Creates ipynb file for country, based on templatename. \n",
    "    File is based in ipynb subfolder of wwwroot.\n",
    "    Returns name of file.\"\"\"\n",
    "    \n",
    "    # create ipynb folder if required\n",
    "    ipynb_dir = os.path.join(wwwroot, \"ipynb\")\n",
    "    if not os.path.exists(ipynb_dir):\n",
    "        os.mkdir(ipynb_dir)\n",
    "        \n",
    "    \n",
    "    check_country_name_is_known(country)\n",
    "    \n",
    "    output_file_name =  f\"{country}.ipynb\"\n",
    "    output_file_path = os.path.join(wwwroot, \"ipynb\", output_file_name)\n",
    "    \n",
    "    # country = sanitize(country)\n",
    "    mappings = {\n",
    "        \"%title%\" : country,\n",
    "        \"%title2%\" : \"\",\n",
    "        \"%country%\" : country,\n",
    "        \"%binderurl%\" : get_binder_url(output_file_name),\n",
    "        \"%create_date%\" : datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    }\n",
    "\n",
    "    modify_template(templatename, os.path.join(\"ipynb\", output_file_name), mappings, wwwroot)\n",
    "    assert os.path.exists(output_file_path), f\"{output_file_path} does not exist\"\n",
    "    return output_file_name\n",
    "\n",
    "def create_ipynb_for_germany(region, subregion, templatename, wwwroot):\n",
    "    \"\"\"Creates ipynb file for region and subregion in Germany, based on templatename. \n",
    "    File is based in ipynb subfolder of wwwroot.\n",
    "    Returns name of file.\"\"\"\n",
    "    germany_check_region_name_is_known(region)\n",
    "    germany_check_subregion_name_is_known(subregion)\n",
    "    \n",
    "    output_file_name =  f\"Germany-{sanitise(region)}-{sanitise(subregion)}.ipynb\"\n",
    "    output_file_path = os.path.join(wwwroot, \"ipynb\", output_file_name)\n",
    "    \n",
    "    # country = sanitize(country)\n",
    "    mappings = {\n",
    "        \"%title%\" : f\"Germany: {subregion} ({region})\",\n",
    "        \"%title2%\" : \"\",\n",
    "        \"%region%\" : region,\n",
    "        \"%subregion%\" : subregion,\n",
    "        \"%binderurl%\" : get_binder_url(output_file_name),\n",
    "        \"%create_date%\" : datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    }\n",
    "\n",
    "    modify_template(templatename, os.path.join(\"ipynb\", output_file_name), mappings, wwwroot)\n",
    "    assert os.path.exists(output_file_path), f\"{output_file_path} does not exist\"\n",
    "    return output_file_name\n",
    "\n",
    "def create_ipynb_for_US(state, subregion, templatename, wwwroot):\n",
    "    \"\"\"Creates ipynb file for states in US, based on templatename. \n",
    "    File is based in ipynb subfolder of wwwroot.\n",
    "    Returns name of file.\"\"\"\n",
    "    assert state in get_US_region_list()\n",
    "    assert subregion == None\n",
    "    \n",
    "    output_file_name =  f\"US-{sanitise(state)}.ipynb\"\n",
    "    output_file_path = os.path.join(wwwroot, \"ipynb\", output_file_name)\n",
    "    \n",
    "    # country = sanitize(country)\n",
    "    mappings = {\n",
    "        \"%title%\" : f\"United States: {state}\",\n",
    "        \"%title2%\" : \"\",\n",
    "        \"%region%\" : state,\n",
    "        \"%subregion%\" : subregion,\n",
    "        \"%binderurl%\" : get_binder_url(output_file_name),\n",
    "        \"%create_date%\" : datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    }\n",
    "\n",
    "    modify_template(templatename, os.path.join(\"ipynb\", output_file_name), mappings, wwwroot)\n",
    "    assert os.path.exists(output_file_path), f\"{output_file_path} does not exist\"\n",
    "    return output_file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_executor = ExecutePreprocessor()\n",
    "nb_executor.allow_errors = True\n",
    "\n",
    "html_exporter = HTMLExporter()\n",
    "html_writer = FilesWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_convert_html(nb_path, outdir):\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    filename = os.path.basename(nb_path)\n",
    "    outpath = os.path.join(outdir, os.path.splitext(filename)[0])\n",
    "    with open(nb_path) as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "        nb = nb_executor.preprocess(nb)[0]\n",
    "        body, resources = html_exporter.from_notebook_node(nb)\n",
    "        html_writer.write(body, resources, outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbconvert_ipynb2html(ipynb_name, wwwroot):\n",
    "    \"\"\"Given the name of a a notebook (such as \"germany.ipynb\"), create the \n",
    "    corresponding html file (\"html/germany.html\") from the notebook file in \n",
    "    \"ipynb\" and return the name of the file (i.e. germany.html).\n",
    "    \"\"\"\n",
    "    ipynb_dir = os.path.join(wwwroot, \"ipynb\")\n",
    "    \n",
    "    # execute notebook and create html copy from it\n",
    "    nb_convert_html(\n",
    "        os.path.join(ipynb_dir, ipynb_name),\n",
    "        os.path.join(wwwroot, \"html\")\n",
    "    )\n",
    "\n",
    "    # compute output path\n",
    "    output_file_name = os.path.splitext(ipynb_name)[0] + \".html\"\n",
    "    assert os.path.exists(os.path.join(wwwroot, \"html\", output_file_name))\n",
    "    \n",
    "    return output_file_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_markdown_index_list(category):\n",
    "    \"\"\"Assemble a markdown table like this:\n",
    "    \n",
    "    | Country/Region                       | Total cases   | Total deaths   |\n",
    "    |:-------------------------------------|:--------------|:---------------|\n",
    "    | [Afghanistan](html/Afghanistan.html) | 1,351         | 43             |\n",
    "    | [Albania](html/Albania.html)         | 678           | 27             |\n",
    "    | [Algeria](html/Algeria.html)         | 3,127         | 415            |\n",
    "    | [Andorra](html/Andorra.html)         | 731           | 40             |\n",
    "    \n",
    "    and return as string.\n",
    "    \"\"\"\n",
    "    \n",
    "    known_categories = [\"world\", \"Germany\", \"US\"]\n",
    "               \n",
    "    # gather data\n",
    "    regions_all = MetadataRegion.get_all_as_dataframe()\n",
    "    if category in known_categories:\n",
    "        # select those we are interested in\n",
    "        regions = regions_all[regions_all['category'] == category]\n",
    "    elif category in [\"all-regions\"]:\n",
    "        regions = regions_all\n",
    "    else:\n",
    "        \n",
    "        raise NotImplementedError(f\"category {category} is unknown.\"+\n",
    "                                  f\" Known values are {known_categories + ['all-regions']}\")\n",
    "    \n",
    "    # sanity check\n",
    "    assert len(regions) >= 4\n",
    "    \n",
    "    # change index to contain URLs and one-line summary in markdown syntax\n",
    "    def compose_md_url(x):\n",
    "        one_line_summary, html = x\n",
    "        if isinstance(html, str):\n",
    "            return \"[\" + one_line_summary + \"](\" + os.path.join('html', html) +\")\"\n",
    "        elif repr(html) == 'nan':   # if html was not produced, then variable html is np.nan\n",
    "            print(f\"Missing html for {one_line_summary} - will not add link to html: \\n{x}\")\n",
    "            return one_line_summary\n",
    "        else:\n",
    "            raise NotImplementedError(\"Don't know how to proceed: \", one_line_summary, html, x)\n",
    "\n",
    "    new_index = regions[['one-line-summary', 'html-file']].apply(compose_md_url, axis=1)\n",
    "    regions2 = regions.set_index(new_index)\n",
    "    regions2.index.name = \"Location\"\n",
    "    \n",
    "    # select columns\n",
    "    regions3 = regions2[['max-cases', 'max-deaths', 'cases-last-week']]\n",
    "    regions4 = regions3.applymap(lambda v: '{:,}'.format(v))  # Thousands comma separator\n",
    "    \n",
    "    # rename columns\n",
    "    rename_dict = {'max-cases' : 'Total cases', \n",
    "                   'max-deaths' : 'Total deaths',\n",
    "                   'cases-last-week' : 'New cases last week'}\n",
    "    regions5 = regions4.rename(columns=rename_dict)\n",
    "\n",
    "    return regions5.to_markdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_list():\n",
    "    d, c = fetch_deaths(), fetch_cases()\n",
    "\n",
    "    countries = d.index\n",
    "    countries2 = c.index\n",
    "    assert (countries2 == countries).all()\n",
    "    \n",
    "    # Here we should identify regions in countries, and process those.\n",
    "    # Instead, as a quick hack to get started, we'll just take one country\n",
    "    # and the current \"get_country\" method will sum over all regions of one country if only \n",
    "    # the country name is given.\n",
    "    \n",
    "    return sorted(countries.drop_duplicates())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_page(sections, rootname, wwwroot):\n",
    "    \"\"\"Sections is dictionary: key is title, value is markdown text\"\"\"\n",
    "    md_file = rootname + \".md\"\n",
    "    \n",
    "    with open(os.path.join(wwwroot, md_file), \"tw\") as f:\n",
    "        for section in sections:\n",
    "            f.write(f\"# {section}\\n\\n\")\n",
    "            f.write(sections[section])\n",
    "    print(f\"Written overview to {md_file}.\")\n",
    "    html_file = rootname + \".html\"\n",
    "    subprocess.check_call(f\"pandoc -t html -o {os.path.join(wwwroot, html_file)} \" +\n",
    "                          f\"{os.path.join(wwwroot, md_file)}\", shell=True)\n",
    "    return html_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_germany_subregion_list():\n",
    "    \"\"\"returns list of subregions (Kreise), \n",
    "    ordered according to (i) Land, then (ii) Kreis\n",
    "    \"\"\"\n",
    "    x = fetch_data_germany()\n",
    "    land_kreis = x[['Bundesland', 'Landkreis']]\n",
    "    ordered = land_kreis.sort_values(['Bundesland', 'Landkreis'])\n",
    "    return list(ordered['Landkreis'].drop_duplicates())\n",
    " \n",
    "\n",
    "@joblib_memory.cache\n",
    "def germany_get_bundesland_from_kreis(kreis):\n",
    "        x = fetch_data_germany()\n",
    "        return x[x['Landkreis'] == kreis].iloc[0]['Bundesland']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_wwwroot_exist(wwwroot):\n",
    "    if not os.path.exists(wwwroot):\n",
    "        msg = \"To put the html into github repo for webhosting, run \"\n",
    "        msg += '\"git clone git@github.com:oscovida/oscovida.github.io.git wwwroot\" or similar'\n",
    "        # os.mkdir(wwwroot)\n",
    "        raise ValueError(f\"directory {wwwroot} missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_html_for_john_hopkins_countries(countries, wwwroot, expiry_hours=2):\n",
    "    \"\"\" Create ipynb for country, and create html from it. Update metadata.\n",
    "    \n",
    "    Arguments:\n",
    "    - countries: list of strings with country names\n",
    "    - wwwroot path to root of webpages\n",
    "    - expiry_hours: if the same data set has been processed within the last expiry_hours hours, \n",
    "      skip the task and leave the files and metadata untouched.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    does_wwwroot_exist(wwwroot)\n",
    "    skipped = 0\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        m = MetadataRegion(country)\n",
    "        if m.last_updated_hours_ago() < expiry_hours:\n",
    "            print(f\"Skipping {country} - was updated {m.last_updated_hours_ago():.1f} hours ago\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "        # metadata to be used when we create html pages \n",
    "        m['source'] = \"CSSE Johns Hopkins\"\n",
    "        m['category'] = \"world\"\n",
    "        cases, deaths, region_label = get_country_data(country)\n",
    "        m['max-deaths'] = int(deaths[-1])\n",
    "        m['max-cases'] = int(cases[-1])\n",
    "        m['region'] = str(None)\n",
    "        m['subregion'] = str(None)\n",
    "        one_line_summary = f\"{country}\"\n",
    "        m['one-line-summary'] = one_line_summary  # used as title in table\n",
    "        \n",
    "        # compute number of infections in last week\n",
    "        m['cases-last-week'] = int(get_cases_last_week(cases))\n",
    "    \n",
    "        try:\n",
    "            print(f\"Processing {i+1}/{len(countries)} [{time.time()-start_time:4.0f}s]\")\n",
    "            ipynb_name = create_ipynb_for_country(country, \"template-country.ipynb\", wwwroot=wwwroot)\n",
    "            html_name = nbconvert_ipynb2html(ipynb_name, wwwroot=wwwroot)\n",
    "            m['html-file'] = html_name\n",
    "            m['ipynb-name'] = ipynb_name\n",
    "            print(f\"Mark {country} as updated\")\n",
    "            m.mark_as_updated()\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {country}\", end='')\n",
    "            print(e)\n",
    "            raise e\n",
    "\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    print(f\"Created {len(countries)-skipped} (skipped {skipped}) notebooks and html versions in \" + \\\n",
    "          f\"{time.time()-start_time} seconds\")\n",
    "    \n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_html_for_countries(countries, wwwroot, pool):\n",
    "    processes = pool._processes\n",
    "    padding = processes - (len(countries) % processes)\n",
    "    countries = countries + ([0] * padding)\n",
    "    per_process = int(len(countries)/processes)\n",
    "    countries_per_process = list(countries[i:i+per_process] for i in range(0, len(countries), per_process))\n",
    "    countries_per_process[-1] = countries_per_process[-1][:-padding]\n",
    "    \n",
    "    tasks = ((c, wwwroot) for c in countries_per_process)\n",
    "    \n",
    "    res = pool.starmap(create_html_for_john_hopkins_countries, tasks)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_markdown_index_page(md_content, title, pelican_file_path, \n",
    "                               save_as, wwwroot, slug=None):\n",
    "    \"\"\"Create pelican markdown file, like this:\n",
    "    \n",
    "    title: Germany\n",
    "    tags: Data, Plots, Germany\n",
    "    save-as: germany\n",
    "    date: 2020-04-11 08:00\n",
    "    \"\"\"\n",
    "\n",
    "    if slug is None:\n",
    "        slug = save_as\n",
    "    \n",
    "    with open(os.path.join(pelican_file_path), \"tw\") as f:\n",
    "        f.write(f\"title: {title}\\n\")\n",
    "        # f.write(f\"category: Data\\n\")  - have stopped using categories (22 April 2020)\n",
    "        f.write(f\"tags: Data, Plots, {title}\\n\")\n",
    "        f.write(f\"save-as: {save_as}\\n\")\n",
    "        f.write(f\"slug: {slug}\\n\")\n",
    "        date_time = datetime.datetime.now().strftime(\"%Y/%m/%d %H:%M\")\n",
    "        f.write(f\"date: {date_time}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(md_content)\n",
    "        f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create country overview for the world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "countries = get_country_list()\n",
    "\n",
    "if cores:\n",
    "    with Pool(cores) as pool:\n",
    "        parallel_html_for_countries(countries, wwwroot, pool)\n",
    "else:\n",
    "    create_html_for_john_hopkins_countries(countries, wwwroot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_md = create_markdown_index_list(\"world\")\n",
    "\n",
    "create_markdown_index_page(index_md, title=TITLE_PREFIX + \" Countries of the world\", \n",
    "                           pelican_file_path=\"pelican/content/countries.md\", save_as=\"countries\", \n",
    "                           wwwroot=wwwroot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create list of Germany data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_html_for_Germany(subregions, wwwroot, expiry_hours=2):\n",
    "    \"\"\"If a data set has been created within expire_hours from now, do not update it, but return \n",
    "    immediately.\"\"\"\n",
    "    does_wwwroot_exist(wwwroot)\n",
    "    start_time = time.time()\n",
    "    skipped = 0\n",
    "    \n",
    "    for i, kreis in enumerate(subregions):\n",
    "        m = MetadataRegion(kreis)\n",
    "        if m.last_updated_hours_ago() < expiry_hours:\n",
    "            print(f\"Skipping {kreis} - was updated {m.last_updated_hours_ago():.1f} hours ago\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "        # metadata to be used when we create html pages \n",
    "        m['source'] = \"Robert Koch Institute\"\n",
    "        m['category'] = \"Germany\"\n",
    "        cases, deaths, region_label = get_country_data(\"Germany\",\n",
    "                                                       subregion=kreis)\n",
    "\n",
    "        m['max-deaths'] = int(deaths[-1])\n",
    "        m['max-cases'] = int(cases[-1])\n",
    "        m['subregion'] = kreis\n",
    "        bundesland = germany_get_bundesland_from_kreis(kreis)\n",
    "        m['region'] = bundesland\n",
    "        one_line_summary = f\"Germany: {bundesland} : {kreis}\"\n",
    "        m['one-line-summary'] = one_line_summary  # used as title in table\n",
    "        # compute number of infections in last week\n",
    "        m['cases-last-week'] = int(get_cases_last_week(cases))\n",
    "    \n",
    "\n",
    "        try:    \n",
    "            print(f\"Processing {i+1}/{len(subregions)} [{time.time()-start_time:4.0f}s]\")\n",
    "            ipynb_name = create_ipynb_for_germany(region=bundesland, subregion=kreis, \n",
    "                                                  templatename=\"template-germany.ipynb\", wwwroot=wwwroot)\n",
    "            html_name = nbconvert_ipynb2html(ipynb_name, wwwroot=wwwroot)\n",
    "            m['html-file'] = html_name\n",
    "            m['ipynb-name'] = ipynb_name\n",
    "            print(f\"Mark {kreis} as updated\")\n",
    "            m.mark_as_updated()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {kreis}\", end='')\n",
    "            print(e)\n",
    "            raise\n",
    "            \n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print(f\"Created {len(subregions)-skipped} (skipped {skipped}) notebooks and html versions in \" + \\\n",
    "          f\"{time.time()-start_time} seconds\")\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_html_for_germany(subregions, wwwroot, pool):\n",
    "    processes = pool._processes\n",
    "    padding = processes - (len(subregions) % processes)\n",
    "    subregions = subregions + ([0] * padding)\n",
    "    per_process = int(len(subregions)/processes)\n",
    "    subregions_per_process = list(subregions[i:i+per_process] for i in range(0, len(subregions), per_process))\n",
    "    subregions_per_process[-1] = subregions_per_process[-1][:-padding]\n",
    "    \n",
    "    tasks = ((c, wwwroot) for c in subregions_per_process)\n",
    "    \n",
    "    pool.starmap(create_html_for_Germany, tasks)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of html (Germany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wwwroot = \"wwwroot\"\n",
    "subregions = get_germany_subregion_list()\n",
    "\n",
    "# data cleaning: on 13 April, we had a Landkreis \"LK Göttingen (alt)\"\n",
    "# with only one data point. This causes plots to fail, because there\n",
    "# is nothing to plot, and then the legend() command failed.\n",
    "# We assume that the RKI labels unusual data with '(alt)', and remove those.\n",
    "\n",
    "alt_data_sets = [x for x in subregions if \"(alt)\" in x.lower()]\n",
    "if len(alt_data_sets) > 0:\n",
    "    print(f\"Removing datasets label with '(alt)': {alt_data_sets}\")\n",
    "    for alt in alt_data_sets:\n",
    "        c, d = germany_get_region(landkreis=alt)\n",
    "        print(f\"  removed: {alt} : len(cases)={len(c)}, len(deaths)={len(d)}\")\n",
    "    subregions = [x for x in subregions if not \"(alt)\" in x.lower()]\n",
    "\n",
    "# Actual calculations\n",
    "    \n",
    "if cores:\n",
    "    with Pool(cores) as pool:\n",
    "        parallel_html_for_germany(subregions, wwwroot, pool)\n",
    "else:\n",
    "    create_html_for_Germany(subregions, wwwroot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_md = create_markdown_index_list(category=\"Germany\")\n",
    "\n",
    "create_markdown_index_page(index_md, title= TITLE_PREFIX + \" Germany\", \n",
    "                           pelican_file_path=\"pelican/content/germany.md\", save_as=\"germany\", \n",
    "                           wwwroot=wwwroot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of html (US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_html_for_US(subregions, wwwroot, pool):\n",
    "    processes = pool._processes\n",
    "    padding = processes - (len(subregions) % processes)\n",
    "    subregions = subregions + ([0] * padding)\n",
    "    per_process = int(len(subregions)/processes)\n",
    "    subregions_per_process = list(subregions[i:i+per_process] for i in range(0, len(subregions), per_process))\n",
    "    subregions_per_process[-1] = subregions_per_process[-1][:-padding]\n",
    "    \n",
    "    tasks = ((c, wwwroot) for c in subregions_per_process)\n",
    "    \n",
    "    pool.starmap(create_html_for_US, tasks)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_html_for_US(states, wwwroot, expiry_hours=2):\n",
    "    \"\"\"If a data set has been created within expire_hours from now, do not update it, but return \n",
    "    immediately.\"\"\"\n",
    "    does_wwwroot_exist(wwwroot)\n",
    "    start_time = time.time()\n",
    "    skipped = 0\n",
    "    \n",
    "    for i, state in enumerate(states):\n",
    "        name = f\"US-{state}\"\n",
    "        m = MetadataRegion(name)\n",
    "        if m.last_updated_hours_ago() < expiry_hours:\n",
    "            print(f\"Skipping {name} - was updated {m.last_updated_hours_ago():.1f} hours ago\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "        # metadata to be used when we create html pages \n",
    "        m['source'] = \"Johns Hopkins University CSSE\"\n",
    "        m['category'] = \"US\"\n",
    "        cases, deaths = get_region_US(state)\n",
    "        m['max-deaths'] = int(deaths[-1])\n",
    "        m['max-cases'] = int(cases[-1])\n",
    "        m['subregion'] = None  # would be county\n",
    "        m['region'] = state\n",
    "        one_line_summary = f\"US: {state}\"\n",
    "        m['one-line-summary'] = one_line_summary  # used as title in table\n",
    "        # compute number of infections in last week\n",
    "        m['cases-last-week'] = int(get_cases_last_week(cases))\n",
    "    \n",
    "\n",
    "        try:    \n",
    "            print(f\"Processing {i+1}/{len(states)} [{time.time()-start_time:4.0f}s]\")\n",
    "            ipynb_name = create_ipynb_for_US(state, subregion=None,\n",
    "                                             templatename=\"template-US.ipynb\", wwwroot=wwwroot)\n",
    "            html_name = nbconvert_ipynb2html(ipynb_name, wwwroot=wwwroot)\n",
    "            m['html-file'] = html_name\n",
    "            m['ipynb-name'] = ipynb_name\n",
    "            print(f\"Mark {name} as updated\")\n",
    "            m.mark_as_updated()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {name}\", end='')\n",
    "            print(e)\n",
    "            raise\n",
    "            \n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print(f\"Created {len(states)-skipped} (skipped {skipped}) notebooks and html versions in \" + \\\n",
    "          f\"{time.time()-start_time} seconds\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "states = get_US_region_list()\n",
    "\n",
    "# Actual calculations\n",
    "    \n",
    "if cores:\n",
    "    with Pool(cores) as pool:\n",
    "        parallel_html_for_US(states, wwwroot, pool)\n",
    "else:\n",
    "    create_html_for_US(states, wwwroot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of html (Pelican) for US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_md = create_markdown_index_list(\"US\")\n",
    "\n",
    "create_markdown_index_page(index_md, title=TITLE_PREFIX + \" United States\", \n",
    "                           pelican_file_path=\"pelican/content/US.md\", save_as=\"us\", \n",
    "                           wwwroot=wwwroot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of html page for all regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_md = create_markdown_index_list(\"all-regions\")\n",
    "create_markdown_index_page(index_md, title=TITLE_PREFIX + \" All regions and countries\", \n",
    "                           pelican_file_path=\"pelican/content/all-regions.md\", save_as=\"all-regions\", \n",
    "                           wwwroot=wwwroot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check what went wrong?\n",
    "- Sometimes, a job times out. If so, the loop below will report this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = MetadataRegion.get_all()\n",
    "for name in ms:\n",
    "    m = MetadataRegion(name)\n",
    "    dt = m.last_updated_hours_ago()\n",
    "    if dt > 2:\n",
    "        print(f\"Problem with '{name}', last update: {dt} ago \")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional debugging - RKI seems old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check when the last data from RKI was updated\n",
    "# d = fetch_data_germany()\n",
    "\n",
    "# d.sort_values(\"Meldedatum\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
