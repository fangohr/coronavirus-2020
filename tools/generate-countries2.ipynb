{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import ipynb_py_convert\n",
    "import nbformat\n",
    "import numpy as np\n",
    "from nbconvert import HTMLExporter\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "from nbconvert.writers import FilesWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Set to false if you do not want multiprocessing enabled\n",
    "cores = 'auto'\n",
    "\n",
    "if cores == 'auto':\n",
    "    cores = max(1, cpu_count())\n",
    "    # try at most 4 to reduce probability of error message like\n",
    "    # the one shown at https://github.com/jupyter/jupyter_client/issues/541\n",
    "    cores = min(cores, 4)\n",
    "\n",
    "\n",
    "if cores:\n",
    "    print(f'Using {cores} processes')\n",
    "    \n",
    "wwwroot = \"wwwroot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "from coronavirus import *\n",
    "from coronavirus import MetadataRegion\n",
    "\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)  #  Disable pandas scientific notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning of cache and copying files has moved to\n",
    "\n",
    "- `generate-webpage-clean-setup.py` and \n",
    "- `generate-webpage-clean-setup.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_PREFIX = \"Tracking plots: \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, c = fetch_deaths(), fetch_cases()\n",
    "\n",
    "countries = d.index\n",
    "countries2 = c.index\n",
    "assert (countries2 == countries).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_US_cases = fetch_cases_US()\n",
    "data_US_deaths = fetch_deaths_US()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also fetch data from Germany, so it is available later from the cache\n",
    "germany = fetch_data_germany()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_wwwroot_exist(wwwroot):\n",
    "    if not os.path.exists(wwwroot):\n",
    "        msg = \"To put the html into github repo for webhosting, run \"\n",
    "        msg += '\"git clone git@github.com:oscovida/oscovida.github.io.git wwwroot\" or similar'\n",
    "        # os.mkdir(wwwroot)\n",
    "        raise ValueError(f\"directory {wwwroot} missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Page Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_markdown_index_list(category):\n",
    "    \"\"\"Assemble a markdown table like this:\n",
    "    \n",
    "    | Country/Region                       | Total cases   | Total deaths   |\n",
    "    |:-------------------------------------|:--------------|:---------------|\n",
    "    | [Afghanistan](html/Afghanistan.html) | 1,351         | 43             |\n",
    "    | [Albania](html/Albania.html)         | 678           | 27             |\n",
    "    | [Algeria](html/Algeria.html)         | 3,127         | 415            |\n",
    "    | [Andorra](html/Andorra.html)         | 731           | 40             |\n",
    "    \n",
    "    and return as string.\n",
    "    \"\"\"\n",
    "    \n",
    "    known_categories = [\"world\", \"Germany\", \"US\"]\n",
    "               \n",
    "    # gather data\n",
    "    regions_all = MetadataRegion.get_all_as_dataframe()\n",
    "    if category in known_categories:\n",
    "        # select those we are interested in\n",
    "        regions = regions_all[regions_all['category'] == category]\n",
    "    elif category in [\"all-regions\"]:\n",
    "        regions = regions_all\n",
    "    else:\n",
    "        \n",
    "        raise NotImplementedError(f\"category {category} is unknown.\"+\n",
    "                                  f\" Known values are {known_categories + ['all-regions']}\")\n",
    "    \n",
    "    # change index to contain URLs and one-line summary in markdown syntax\n",
    "    def compose_md_url(x):\n",
    "        one_line_summary, html = x\n",
    "        if isinstance(html, str):\n",
    "            return \"[\" + one_line_summary + \"](\" + os.path.join('html', html) +\")\"\n",
    "        elif repr(html) == 'nan':   # if html was not produced, then variable html is np.nan\n",
    "            print(f\"Missing html for {one_line_summary} - will not add link to html: \\n{x}\")\n",
    "            return one_line_summary\n",
    "        else:\n",
    "            raise NotImplementedError(\"Don't know how to proceed: \", one_line_summary, html, x)\n",
    "\n",
    "    new_index = regions[['one-line-summary', 'html-file']].apply(compose_md_url, axis=1)\n",
    "    regions2 = regions.set_index(new_index)\n",
    "    regions2.index.name = \"Location\"\n",
    "    \n",
    "    # select columns\n",
    "    regions3 = regions2[['max-cases', 'max-deaths', 'cases-last-week']]\n",
    "    regions4 = regions3.applymap(lambda v: '{:,}'.format(v))  # Thousands comma separator\n",
    "    \n",
    "    # rename columns\n",
    "    rename_dict = {'max-cases' : 'Total cases', \n",
    "                   'max-deaths' : 'Total deaths',\n",
    "                   'cases-last-week' : 'New cases last week'}\n",
    "    regions5 = regions4.rename(columns=rename_dict)\n",
    "\n",
    "    return regions5.to_markdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_markdown_index_page(md_content, title, pelican_file_path, \n",
    "                               save_as, wwwroot, slug=None):\n",
    "    \"\"\"Create pelican markdown file, like this:\n",
    "    \n",
    "    title: Germany\n",
    "    tags: Data, Plots, Germany\n",
    "    save-as: germany\n",
    "    date: 2020-04-11 08:00\n",
    "    \"\"\"\n",
    "\n",
    "    if slug is None:\n",
    "        slug = save_as\n",
    "    \n",
    "    with open(os.path.join(pelican_file_path), \"tw\") as f:\n",
    "        f.write(f\"title: {title}\\n\")\n",
    "        # f.write(f\"category: Data\\n\")  - have stopped using categories (22 April 2020)\n",
    "        f.write(f\"tags: Data, Plots, {title}\\n\")\n",
    "        f.write(f\"save-as: {save_as}\\n\")\n",
    "        f.write(f\"slug: {slug}\\n\")\n",
    "        date_time = datetime.datetime.now().strftime(\"%Y/%m/%d %H:%M\")\n",
    "        f.write(f\"date: {date_time}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(md_content)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_page(sections, rootname, wwwroot):\n",
    "    \"\"\"Sections is dictionary: key is title, value is markdown text\"\"\"\n",
    "    md_file = rootname + \".md\"\n",
    "    \n",
    "    with open(os.path.join(wwwroot, md_file), \"tw\") as f:\n",
    "        for section in sections:\n",
    "            f.write(f\"# {section}\\n\\n\")\n",
    "            f.write(sections[section])\n",
    "    print(f\"Written overview to {md_file}.\")\n",
    "    html_file = rootname + \".html\"\n",
    "    subprocess.check_call(f\"pandoc -t html -o {os.path.join(wwwroot, html_file)} \" +\n",
    "                          f\"{os.path.join(wwwroot, md_file)}\", shell=True)\n",
    "    return html_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract Report Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseReport:\n",
    "    def __init__(self, country, title, overview_function, overview_args,\n",
    "                 data_load_function, data_load_args, output_file, wwwroot):\n",
    "        self.country = country\n",
    "        self.title = title\n",
    "\n",
    "        self.overview_function = overview_function\n",
    "        self.overview_args = overview_args\n",
    "\n",
    "        self.data_load_function = data_load_function\n",
    "        self.data_load_args = data_load_args\n",
    "\n",
    "        self.output_file_name = self.sanitise(output_file) + \".ipynb\"\n",
    "        self.output_ipynb_path = os.path.join(\n",
    "            wwwroot, \"ipynb\", self.output_file_name)\n",
    "        self.output_html_path = os.path.join(\n",
    "            wwwroot, \"html\", self.output_file_name.replace(\".ipynb\", \".html\"))\n",
    "\n",
    "        self.create_date = datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "    @staticmethod\n",
    "    def sanitise(name):\n",
    "        \"\"\"Given a country name as a string, sanitise it for use as URL and\n",
    "        filename by getting rid of spaces and commas\n",
    "\n",
    "        return cleaned string.\n",
    "\n",
    "        (Leave umlauts for now)\n",
    "        \"\"\"\n",
    "        s = name.replace(\" \", \"-\")\n",
    "        s = s.replace(\",\", \"-\")\n",
    "        return s\n",
    "\n",
    "    @property\n",
    "    def get_binder_url(self):\n",
    "        \"\"\"Given a notebook name, compute the path\"\"\"\n",
    "        base = \"https://mybinder.org/v2/gh/oscovida/binder/master?filepath=ipynb/\"\n",
    "        return base + self.output_file_name.replace(\" \", \"%20\")\n",
    "\n",
    "    @property\n",
    "    def mapping(self):\n",
    "        return {\n",
    "            \"TITLE\": self.title,\n",
    "            \"COUNTRY\": self.country,\n",
    "            \"BINDER_URL\": self.get_binder_url,\n",
    "            \"CREATION_DATE\" : self.create_date,\n",
    "            \"OVERVIEW_FUNCTION\": self.overview_function,\n",
    "            \"OVERVIEW_ARGS\": self.overview_args,\n",
    "            \"DATA_LOAD_FUNCTION\": self.data_load_function,\n",
    "            \"DATA_LOAD_ARGS\": self.data_load_args\n",
    "        }\n",
    "\n",
    "    def generate_notebook(self, templatefile=\"./template-report.py\"):\n",
    "        with open(templatefile, 'r') as f:\n",
    "            template_str = f.read()\n",
    "\n",
    "        template_str = template_str.format_map(self.mapping)\n",
    "\n",
    "        notebook = ipynb_py_convert.py2nb(template_str)\n",
    "\n",
    "        with open(self.output_ipynb_path, 'tw') as f:\n",
    "            print(self.output_ipynb_path)\n",
    "            json.dump(notebook, f, indent=2)\n",
    "\n",
    "            print(f\"Written file to {self.output_file_name}\")\n",
    "\n",
    "    def generate_html(self, kernel_name='python3'):\n",
    "        nb_executor = ExecutePreprocessor(kernel_name=kernel_name)\n",
    "        nb_executor.allow_errors = True\n",
    "\n",
    "        html_exporter = HTMLExporter()\n",
    "        html_writer = FilesWriter()\n",
    "\n",
    "        with open(self.output_ipynb_path) as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "            nb = nb_executor.preprocess(nb)[0]\n",
    "            body, resources = html_exporter.from_notebook_node(nb)\n",
    "            #  HTML writer automatically adds .html to the end, so get rid of it\n",
    "            html_writer.write(body, resources,\n",
    "                self.output_html_path.replace(\".html\", \"\"))\n",
    "\n",
    "            print(f\"Written file to {self.output_html_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Country(BaseReport):\n",
    "    def __init__(self, country, wwwroot='wwwroot'):\n",
    "        title = country\n",
    "        overview_function = \"overview\"\n",
    "        overview_args = f\"\\\"{country}\\\"\"\n",
    "        data_load_function = \"get_country_data\"\n",
    "        data_load_args = f\"\\\"{country}\\\"\"\n",
    "        output_file = f\"{country}\"\n",
    "\n",
    "        self.check_country_is_known(country)\n",
    "\n",
    "        super().__init__(country, title, overview_function, overview_args,\n",
    "                         data_load_function, data_load_args, output_file, wwwroot)\n",
    "\n",
    "    @staticmethod\n",
    "    def check_country_is_known(country):\n",
    "        d = fetch_deaths()\n",
    "        assert country in d.index, f\"{country} is unknown. Known countries are {sorted(d.index)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_list():\n",
    "    d, c = fetch_deaths(), fetch_cases()\n",
    "\n",
    "    countries = d.index\n",
    "    countries2 = c.index\n",
    "    assert (countries2 == countries).all()\n",
    "    \n",
    "    # Here we should identify regions in countries, and process those.\n",
    "    # Instead, as a quick hack to get started, we'll just take one country\n",
    "    # and the current \"get_country\" method will sum over all regions of one country if only \n",
    "    # the country name is given.\n",
    "    \n",
    "    return sorted(countries.drop_duplicates())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_html_for_john_hopkins_countries(countries, wwwroot, expiry_hours=2):\n",
    "    \"\"\" Create ipynb for country, and create html from it. Update metadata.\n",
    "    \n",
    "    Arguments:\n",
    "    - countries: list of strings with country names\n",
    "    - wwwroot path to root of webpages\n",
    "    - expiry_hours: if the same data set has been processed within the last expiry_hours hours, \n",
    "      skip the task and leave the files and metadata untouched.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    does_wwwroot_exist(wwwroot)\n",
    "    skipped = 0\n",
    "\n",
    "    for i, country in enumerate(countries):\n",
    "        m = MetadataRegion(country)\n",
    "#         if m.last_updated_hours_ago() < expiry_hours:\n",
    "#             print(f\"Skipping {country} - was updated {m.last_updated_hours_ago():.1f} hours ago\")\n",
    "#             skipped += 1\n",
    "#             continue\n",
    "            \n",
    "        # metadata to be used when we create html pages \n",
    "        m['source'] = \"CSSE Johns Hopkins\"\n",
    "        m['category'] = \"world\"\n",
    "        cases, deaths, region_label = get_country_data(country)\n",
    "        m['max-deaths'] = int(deaths[-1])\n",
    "        m['max-cases'] = int(cases[-1])\n",
    "        m['region'] = str(None)\n",
    "        m['subregion'] = str(None)\n",
    "        one_line_summary = f\"{country}\"\n",
    "        m['one-line-summary'] = one_line_summary  # used as title in table\n",
    "        \n",
    "        # compute number of infections in last week\n",
    "        m['cases-last-week'] = int(get_cases_last_week(cases))\n",
    "    \n",
    "        try:\n",
    "            print(f\"Processing {i+1}/{len(countries)} [{time.time()-start_time:4.0f}s]\")\n",
    "            country = Country(country, wwwroot=wwwroot)\n",
    "            country.generate_notebook()\n",
    "            country.generate_html()\n",
    "            m['html-file'] = country.output_ipynb_path\n",
    "            m['ipynb-name'] = country.output_html_path\n",
    "            print(f\"Mark {country.title} as updated\")\n",
    "            m.mark_as_updated()\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {country}\", end='')\n",
    "            print(e)\n",
    "            raise e\n",
    "\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    print(f\"Created {len(countries)-skipped} (skipped {skipped}) notebooks and html versions in \" + \\\n",
    "          f\"{time.time()-start_time} seconds\")\n",
    "    \n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_html_for_countries(countries, wwwroot, pool):\n",
    "    processes = pool._processes\n",
    "    padding = processes - (len(countries) % processes)\n",
    "    countries = countries + ([0] * padding)\n",
    "    per_process = int(len(countries)/processes)\n",
    "    countries_per_process = list(countries[i:i+per_process] for i in range(0, len(countries), per_process))\n",
    "    countries_per_process[-1] = countries_per_process[-1][:-padding]\n",
    "    \n",
    "    tasks = ((c, wwwroot) for c in countries_per_process)\n",
    "    \n",
    "    res = pool.starmap(create_html_for_john_hopkins_countries, tasks)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = get_country_list()\n",
    "\n",
    "countries = countries[0:8]\n",
    "cores=False\n",
    "\n",
    "if cores:\n",
    "    with Pool(cores) as pool:\n",
    "        parallel_html_for_countries(countries, wwwroot, pool)\n",
    "else:\n",
    "    create_html_for_john_hopkins_countries(countries, wwwroot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_md = create_markdown_index_list(\"world\")\n",
    "\n",
    "create_markdown_index_page(index_md, title=TITLE_PREFIX + \" Countries of the world\", \n",
    "                           pelican_file_path=\"pelican/content/countries.md\", save_as=\"countries\", \n",
    "                           wwwroot=wwwroot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Germany Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Germany(BaseReport):\n",
    "    def __init__(self, region, subregion, wwwroot='wwwroot'):\n",
    "        country = \"Germany\"\n",
    "        title = f\"{country}: {subregion} ({region})\"\n",
    "        overview_function = \"overview\"\n",
    "        overview_args = f\"country=\\\"{country}\\\", subregion=\\\"{subregion}\\\"\"\n",
    "        data_load_function = \"germany_get_region\"\n",
    "        data_load_args = f\"landkreis=\\\"{subregion}\\\"\"\n",
    "        output_file = f\"Germany-{region}-{subregion}\"\n",
    "\n",
    "        self.germany_check_region_is_known(region)\n",
    "        self.germany_check_subregion__is_known(subregion)\n",
    "\n",
    "        super().__init__(country, title, overview_function, overview_args,\n",
    "                         data_load_function, data_load_args, output_file, wwwroot)\n",
    "\n",
    "    @staticmethod\n",
    "    def germany_check_region_is_known(region):\n",
    "        d = fetch_data_germany()\n",
    "        assert region in list(d['Bundesland'].drop_duplicates()), \\\n",
    "            f\"{region} is unknown.\"\n",
    "\n",
    "    @staticmethod\n",
    "    def germany_check_subregion__is_known(subregion):\n",
    "        d = fetch_data_germany()\n",
    "        assert subregion in list(d['Landkreis'].drop_duplicates()), \\\n",
    "            f\"{subregion} is unknown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_germany_subregion_list():\n",
    "    \"\"\"returns list of subregions (Kreise), \n",
    "    ordered according to (i) Land, then (ii) Kreis\n",
    "    \"\"\"\n",
    "    x = fetch_data_germany()\n",
    "    land_kreis = x[['Bundesland', 'Landkreis']]\n",
    "    ordered = land_kreis.sort_values(['Bundesland', 'Landkreis'])\n",
    "    return list(ordered['Landkreis'].drop_duplicates())\n",
    " \n",
    "\n",
    "@joblib_memory.cache\n",
    "def germany_get_bundesland_from_kreis(kreis):\n",
    "        x = fetch_data_germany()\n",
    "        return x[x['Landkreis'] == kreis].iloc[0]['Bundesland']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_html_for_Germany(subregions, wwwroot, expiry_hours=2):\n",
    "    \"\"\"If a data set has been created within expire_hours from now, do not update it, but return \n",
    "    immediately.\"\"\"\n",
    "    does_wwwroot_exist(wwwroot)\n",
    "    start_time = time.time()\n",
    "    skipped = 0\n",
    "    \n",
    "    for i, kreis in enumerate(subregions):\n",
    "        m = MetadataRegion(kreis)\n",
    "#         if m.last_updated_hours_ago() < expiry_hours:\n",
    "#             print(f\"Skipping {kreis} - was updated {m.last_updated_hours_ago():.1f} hours ago\")\n",
    "#             skipped += 1\n",
    "#             continue\n",
    "            \n",
    "        # metadata to be used when we create html pages \n",
    "        m['source'] = \"Robert Koch Institute\"\n",
    "        m['category'] = \"Germany\"\n",
    "        cases, deaths, region_label = get_country_data(\"Germany\",\n",
    "                                                       subregion=kreis)\n",
    "\n",
    "        m['max-deaths'] = int(deaths[-1])\n",
    "        m['max-cases'] = int(cases[-1])\n",
    "        m['subregion'] = kreis\n",
    "        bundesland = germany_get_bundesland_from_kreis(kreis)\n",
    "        m['region'] = bundesland\n",
    "        one_line_summary = f\"Germany: {bundesland} : {kreis}\"\n",
    "        m['one-line-summary'] = one_line_summary  # used as title in table\n",
    "        # compute number of infections in last week\n",
    "        m['cases-last-week'] = int(get_cases_last_week(cases))\n",
    "\n",
    "        try:    \n",
    "            print(f\"Processing {i+1}/{len(subregions)} [{time.time()-start_time:4.0f}s]\")\n",
    "            germany = Germany(bundesland, kreis, wwwroot=wwwroot)\n",
    "            germany.generate_notebook()\n",
    "            germany.generate_html()\n",
    "            m['html-file'] = germany.output_html_path\n",
    "            m['ipynb-name'] = germany.output_ipynb_path\n",
    "            print(f\"Mark {germany.title} as updated\")\n",
    "            m.mark_as_updated()\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {kreis}\", end='')\n",
    "            print(e)\n",
    "            raise\n",
    "            \n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print(f\"Created {len(subregions)-skipped} (skipped {skipped}) notebooks and html versions in \" + \\\n",
    "          f\"{time.time()-start_time} seconds\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_html_for_germany(subregions, wwwroot, pool):\n",
    "    processes = pool._processes\n",
    "    padding = processes - (len(subregions) % processes)\n",
    "    subregions = subregions + ([0] * padding)\n",
    "    per_process = int(len(subregions)/processes)\n",
    "    subregions_per_process = list(subregions[i:i+per_process] for i in range(0, len(subregions), per_process))\n",
    "    subregions_per_process[-1] = subregions_per_process[-1][:-padding]\n",
    "    \n",
    "    tasks = ((c, wwwroot) for c in subregions_per_process)\n",
    "    \n",
    "    pool.starmap(create_html_for_Germany, tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wwwroot = \"wwwroot\"\n",
    "subregions = get_germany_subregion_list()\n",
    "\n",
    "# data cleaning: on 13 April, we had a Landkreis \"LK Göttingen (alt)\"\n",
    "# with only one data point. This causes plots to fail, because there\n",
    "# is nothing to plot, and then the legend() command failed.\n",
    "# We assume that the RKI labels unusual data with '(alt)', and remove those.\n",
    "\n",
    "alt_data_sets = [x for x in subregions if \"(alt)\" in x.lower()]\n",
    "if len(alt_data_sets) > 0:\n",
    "    print(f\"Removing datasets label with '(alt)': {alt_data_sets}\")\n",
    "    for alt in alt_data_sets:\n",
    "        c, d = germany_get_region(landkreis=alt)\n",
    "        print(f\"  removed: {alt} : len(cases)={len(c)}, len(deaths)={len(d)}\")\n",
    "    subregions = [x for x in subregions if not \"(alt)\" in x.lower()]\n",
    "\n",
    "# Actual calculations\n",
    "\n",
    "subregions = subregions[0:8]\n",
    "cores=2\n",
    "    \n",
    "if cores:\n",
    "    with Pool(cores) as pool:\n",
    "        parallel_html_for_germany(subregions, wwwroot, pool)\n",
    "else:\n",
    "    create_html_for_Germany(subregions, wwwroot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_md = create_markdown_index_list(category=\"Germany\")\n",
    "\n",
    "create_markdown_index_page(index_md, title= TITLE_PREFIX + \" Germany\", \n",
    "                           pelican_file_path=\"pelican/content/germany.md\", save_as=\"germany\", \n",
    "                           wwwroot=wwwroot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USA Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class USA(BaseReport):\n",
    "    def __init__(self, region, wwwroot='wwwroot'):\n",
    "        country = \"USA\"\n",
    "        title = f\"United States: {region}\"\n",
    "        overview_function = \"overview\"\n",
    "        overview_args = f\"country=\\\"US\\\", region=\\\"{region}\\\"\"\n",
    "        data_load_function = \"get_country_data\"\n",
    "        data_load_args = f\"\\\"US\\\", \\\"{region}\\\"\"\n",
    "        output_file = f\"US-{region}\"\n",
    "\n",
    "        super().__init__(country, title, overview_function, overview_args,\n",
    "                         data_load_function, data_load_args, output_file, wwwroot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_html_for_US(subregions, wwwroot, pool):\n",
    "    processes = pool._processes\n",
    "    padding = processes - (len(subregions) % processes)\n",
    "    subregions = subregions + ([0] * padding)\n",
    "    per_process = int(len(subregions)/processes)\n",
    "    subregions_per_process = list(subregions[i:i+per_process] for i in range(0, len(subregions), per_process))\n",
    "    subregions_per_process[-1] = subregions_per_process[-1][:-padding]\n",
    "    \n",
    "    tasks = ((c, wwwroot) for c in subregions_per_process)\n",
    "    \n",
    "    pool.starmap(create_html_for_US, tasks)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_html_for_US(states, wwwroot, expiry_hours=2):\n",
    "    \"\"\"If a data set has been created within expire_hours from now, do not update it, but return \n",
    "    immediately.\"\"\"\n",
    "    does_wwwroot_exist(wwwroot)\n",
    "    start_time = time.time()\n",
    "    skipped = 0\n",
    "    \n",
    "    for i, state in enumerate(states):\n",
    "        name = f\"US-{state}\"\n",
    "        m = MetadataRegion(name)\n",
    "#         if m.last_updated_hours_ago() < expiry_hours:\n",
    "#             print(f\"Skipping {name} - was updated {m.last_updated_hours_ago():.1f} hours ago\")\n",
    "#             skipped += 1\n",
    "#             continue\n",
    "            \n",
    "        # metadata to be used when we create html pages \n",
    "        m['source'] = \"Johns Hopkins University CSSE\"\n",
    "        m['category'] = \"US\"\n",
    "        cases, deaths = get_region_US(state)\n",
    "        m['max-deaths'] = int(deaths[-1])\n",
    "        m['max-cases'] = int(cases[-1])\n",
    "        m['subregion'] = None  # would be county\n",
    "        m['region'] = state\n",
    "        one_line_summary = f\"US: {state}\"\n",
    "        m['one-line-summary'] = one_line_summary  # used as title in table\n",
    "        # compute number of infections in last week\n",
    "        m['cases-last-week'] = int(get_cases_last_week(cases))\n",
    "    \n",
    "\n",
    "        try:    \n",
    "            print(f\"Processing {i+1}/{len(states)} [{time.time()-start_time:4.0f}s]\")\n",
    "            usa = USA(state, wwwroot=wwwroot)\n",
    "            usa.generate_notebook()\n",
    "            usa.generate_html()\n",
    "            m['html-file'] = usa.output_ipynb_path\n",
    "            m['ipynb-name'] = usa.output_html_path\n",
    "            print(f\"Mark {usa.title} as updated\")\n",
    "            m.mark_as_updated()\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {name}\", end='')\n",
    "            print(e)\n",
    "            raise\n",
    "            \n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print(f\"Created {len(states)-skipped} (skipped {skipped}) notebooks and html versions in \" + \\\n",
    "          f\"{time.time()-start_time} seconds\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = get_US_region_list()\n",
    "\n",
    "# Actual calculations\n",
    "states = states[0:8]\n",
    "cores=2\n",
    "\n",
    "if cores:\n",
    "    with Pool(cores) as pool:\n",
    "        parallel_html_for_US(states, wwwroot, pool)\n",
    "else:\n",
    "    create_html_for_US(states, wwwroot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oscovida",
   "language": "python",
   "name": "oscovida"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
